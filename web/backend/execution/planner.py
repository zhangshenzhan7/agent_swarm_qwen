"""Supervisor è§„åˆ’æ­¥éª¤æ‰§è¡Œ - é€šè¿‡ TaskBoard + WaveExecutor å®ç°äº‹ä»¶é©±åŠ¨çš„åŠ¨æ€æ³¢æ¬¡æ‰§è¡Œ"""

import asyncio
import json
import logging
import os
import shutil
import subprocess
import tempfile
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple, Set

from src import TaskStatus, AgentStatus, PREDEFINED_ROLES
from src.models.enums import OutputType
from src.output_registry import OutputTypeRegistry
from src.output_pipeline import OutputPipeline
from src.artifact_storage import ArtifactStorage
from src.handlers import register_all_handlers
from state import state
from utils import clean_thinking_tags
from execution.report import generate_final_report
from execution.helpers import analyze_dependency_layers, map_role_hint_to_key

# ä¸Šä¼ ç›®å½•
UPLOAD_DIR = os.path.join(os.path.dirname(__file__), '..', 'uploads')

logger = logging.getLogger(__name__)


async def _merge_video_segments(
    video_urls: List[str], task_id: str, log_event
) -> Optional[str]:
    """ä¸‹è½½å¤šæ®µè§†é¢‘å¹¶ç”¨ FFmpeg åˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶ã€‚

    Returns:
        åˆå¹¶åè§†é¢‘çš„æœ¬åœ° URLï¼ˆå¦‚ /api/files/merged_xxx.mp4ï¼‰ï¼Œå¤±è´¥è¿”å› Noneã€‚
    """
    if not shutil.which("ffmpeg"):
        await log_event("âš ï¸ æœªæ£€æµ‹åˆ° ffmpegï¼Œæ— æ³•åˆå¹¶è§†é¢‘", "warning")
        return None

    os.makedirs(UPLOAD_DIR, exist_ok=True)
    tmp_dir = tempfile.mkdtemp(prefix="video_merge_")

    try:
        import aiohttp

        # 1. ä¸‹è½½æ‰€æœ‰è§†é¢‘ç‰‡æ®µ
        downloaded: List[str] = []
        async with aiohttp.ClientSession() as session:
            for i, url in enumerate(video_urls):
                if not url:
                    continue
                seg_path = os.path.join(tmp_dir, f"seg_{i:03d}.mp4")
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=120)) as resp:
                        if resp.status == 200:
                            with open(seg_path, "wb") as f:
                                f.write(await resp.read())
                            downloaded.append(seg_path)
                        else:
                            await log_event(f"âš ï¸ ä¸‹è½½è§†é¢‘ç‰‡æ®µ {i+1} å¤±è´¥: HTTP {resp.status}", "warning")
                except Exception as e:
                    await log_event(f"âš ï¸ ä¸‹è½½è§†é¢‘ç‰‡æ®µ {i+1} å¼‚å¸¸: {str(e)[:60]}", "warning")

        if len(downloaded) < 2:
            await log_event("âš ï¸ å¯ç”¨è§†é¢‘ç‰‡æ®µä¸è¶³ï¼Œè·³è¿‡åˆå¹¶", "warning")
            return None

        # 2. ç”Ÿæˆ FFmpeg concat æ–‡ä»¶åˆ—è¡¨
        list_path = os.path.join(tmp_dir, "filelist.txt")
        with open(list_path, "w") as f:
            for seg in downloaded:
                f.write(f"file '{seg}'\n")

        # 3. ç”¨ FFmpeg concat demuxer åˆå¹¶
        merged_filename = f"merged_{task_id[:8]}_{uuid.uuid4().hex[:6]}.mp4"
        merged_path = os.path.join(UPLOAD_DIR, merged_filename)

        loop = asyncio.get_event_loop()
        cmd = [
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", list_path, "-c", "copy", merged_path,
        ]

        def run_ffmpeg():
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=120
            )
            return result.returncode, result.stderr

        returncode, stderr = await loop.run_in_executor(None, run_ffmpeg)

        if returncode != 0:
            # concat copy å¤±è´¥æ—¶å°è¯•é‡æ–°ç¼–ç 
            await log_event("âš ï¸ è§†é¢‘ç›´æ¥æ‹¼æ¥å¤±è´¥ï¼Œå°è¯•é‡æ–°ç¼–ç åˆå¹¶...", "warning")
            cmd_reencode = [
                "ffmpeg", "-y", "-f", "concat", "-safe", "0",
                "-i", list_path,
                "-c:v", "libx264", "-preset", "fast", "-crf", "23",
                "-c:a", "aac", "-b:a", "128k",
                merged_path,
            ]

            def run_ffmpeg_reencode():
                result = subprocess.run(
                    cmd_reencode, capture_output=True, text=True, timeout=300
                )
                return result.returncode, result.stderr

            returncode, stderr = await loop.run_in_executor(None, run_ffmpeg_reencode)

        if returncode == 0 and os.path.exists(merged_path):
            return f"/api/files/{merged_filename}"
        else:
            logger.error(f"FFmpeg åˆå¹¶å¤±è´¥: {stderr[:200]}")
            await log_event(f"âš ï¸ FFmpeg åˆå¹¶å¤±è´¥: {stderr[:80]}", "warning")
            return None

    except ImportError:
        await log_event("âš ï¸ ç¼ºå°‘ aiohttp åº“ï¼Œæ— æ³•ä¸‹è½½è§†é¢‘ç‰‡æ®µè¿›è¡Œåˆå¹¶", "warning")
        return None
    except Exception as e:
        logger.error(f"è§†é¢‘åˆå¹¶å¼‚å¸¸: {e}")
        await log_event(f"âš ï¸ è§†é¢‘åˆå¹¶å¼‚å¸¸: {str(e)[:60]}", "warning")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(tmp_dir, ignore_errors=True)


async def execute_with_supervisor_plan(task_id: str, task: Dict, content: str, metadata: Optional[Dict], log_event, update_stage, stage_offset: int):
    """
    ä½¿ç”¨ Supervisor è§„åˆ’çš„æ­¥éª¤æ‰§è¡Œä»»åŠ¡
    é€šè¿‡ TaskBoard + WaveExecutor å®ç°äº‹ä»¶é©±åŠ¨çš„åŠ¨æ€æ³¢æ¬¡æ‰§è¡Œ
    """
    from src.qwen.models import Message, QwenConfig
    from src.task_board import TaskBoard
    from src.wave_executor import WaveExecutor
    from src.models.task import SubTask
    from src.models.team import TaskBoardStatus

    steps = task["plan"]["execution_flow"]["steps"]
    step_list = list(steps.values())

    # ========== é˜¶æ®µ 1: ä»»åŠ¡åˆ†æ ==========
    task["status"] = TaskStatus.ANALYZING.value
    await update_stage(stage_offset, "running")
    await log_event("ğŸ” æ­£åœ¨åˆ†æä»»åŠ¡...")

    # åˆ†æä¾èµ–å±‚çº§ï¼ˆç”¨äºæ—¥å¿—å±•ç¤ºï¼‰
    dep_layers = analyze_dependency_layers(step_list)
    await update_stage(stage_offset, "completed", f"åˆ†æå®Œæˆï¼Œ{len(step_list)} ä¸ªæ­¥éª¤")
    await log_event(f"âœ… ä»»åŠ¡åˆ†æå®Œæˆï¼Œ{len(step_list)} ä¸ªæ­¥éª¤ï¼Œ{len(dep_layers)} å±‚æ‰§è¡Œæµç¨‹")

    # ========== é˜¶æ®µ 2: ä»»åŠ¡åˆ†è§£ â†’ å‘å¸ƒåˆ° TaskBoard ==========
    task["status"] = TaskStatus.DECOMPOSING.value
    await update_stage(stage_offset + 1, "running")
    await log_event("ğŸ”§ æ­£åœ¨å‡†å¤‡æ‰§è¡Œè®¡åˆ’ï¼Œå‘å¸ƒä»»åŠ¡åˆ°å…±äº«ä»»åŠ¡æ¿...")

    # å°† Supervisor æ­¥éª¤è½¬æ¢ä¸º SubTask å¯¹è±¡
    subtasks = []
    dependencies_map: Dict[str, Set[str]] = {}

    for step in step_list:
        step_id = step.get("step_id")
        deps = step.get("dependencies", [])
        valid_deps = set(d for d in deps if d in steps)

        agent_type = step.get("agent_type", "researcher")
        role_key = map_role_hint_to_key(agent_type)
        if not role_key:
            role_key = "researcher"

        subtask = SubTask(
            id=step_id,
            parent_task_id=task_id,
            content=step.get("description", step.get("name", "æ‰§è¡Œä»»åŠ¡")),
            role_hint=role_key,
            dependencies=valid_deps,
            priority=step.get("step_number", 0),
            estimated_complexity=1.0,
        )
        subtasks.append(subtask)
        dependencies_map[step_id] = valid_deps

        # åˆå§‹åŒ–æ­¥éª¤çŠ¶æ€
        if valid_deps:
            steps[step_id]["status"] = "waiting"
        else:
            steps[step_id]["status"] = "pending"

    # åˆ›å»º TaskBoard å¹¶å‘å¸ƒä»»åŠ¡
    task_board = TaskBoard()
    await task_board.publish_tasks(subtasks, dependencies_map)

    # ========== åˆ›å»ºè´¨é‡é—¨æ§è¯„å®¡å™¨ ==========
    quality_gate_reviewer = None
    supervisor_instance = state.active_supervisors.get(task_id)
    supervisor_config = state.supervisor_config
    if (supervisor_instance and supervisor_config 
            and getattr(supervisor_config, 'enable_quality_gates', False)):
        from src.core.supervisor.quality_gate import QualityGateReviewer
        from src.core.supervisor.flow import ExecutionFlow as CoreExecutionFlow
        # åˆ›å»ºä¸€ä¸ªè½»é‡ ExecutionFlow ç”¨äºè´¨é‡é—¨æ§è¿½è¸ª
        core_flow = CoreExecutionFlow()
        quality_gate_reviewer = QualityGateReviewer(
            supervisor=supervisor_instance,
            config=supervisor_config,
            execution_flow=core_flow,
            task_board=task_board,
        )
        await log_event("ğŸ” è´¨é‡é—¨æ§å·²å¯ç”¨")

    # å¹¿æ’­åˆå§‹çŠ¶æ€
    await state.broadcast("task_updated", task)

    await update_stage(stage_offset + 1, "completed", f"å‘å¸ƒ {len(step_list)} ä¸ªä»»åŠ¡åˆ°ä»»åŠ¡æ¿")
    await log_event(f"ğŸ“Š ä¾èµ–åˆ†æ: {len(dep_layers)} å±‚æ‰§è¡Œæµç¨‹")
    for i, layer in enumerate(dep_layers):
        layer_names = [s.get("name", s.get("step_id")) for s in layer]
        await log_event(f"   ç¬¬ {i+1} å±‚: {', '.join(layer_names)}")

    # ========== é˜¶æ®µ 3: æ™ºèƒ½ä½“åˆ†é… ==========
    await update_stage(stage_offset + 2, "running")
    await log_event("ğŸ‘¥ æ­£åœ¨åˆ†é…æ™ºèƒ½ä½“ï¼ˆWaveExecutor äº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼‰...")
    await update_stage(stage_offset + 2, "completed", f"å°±ç»ªï¼Œ{len(dep_layers)} å±‚åŠ¨æ€æ³¢æ¬¡")

    # ========== é˜¶æ®µ 4: å¹¶è¡Œæ‰§è¡Œï¼ˆWaveExecutor äº‹ä»¶é©±åŠ¨ï¼‰==========
    task["status"] = TaskStatus.EXECUTING.value
    await update_stage(stage_offset + 3, "running")

    # å­˜å‚¨æ­¥éª¤ç»“æœ
    step_results: Dict[str, Any] = {}
    step_agent_mapping: Dict[str, str] = {}
    failed_steps: Set[str] = set()

    # ========== å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡æ‰§è¡Œå‡½æ•° ==========
    async def execute_multimodal_step(
        role_key: str,
        step_desc: str,
        input_context: str,
        instance: Dict,
        step: Dict,
        log_event
    ) -> Tuple[bool, str, str]:
        """æ‰§è¡Œå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼Œè¾“å‡ºç»Ÿä¸€ä¸º JSON å­—ç¬¦ä¸²"""
        import json as _json

        def _extract_prompt_from_context(ctx: str, fallback: str) -> str:
            """ä»ä¸Šæ¸¸ JSON ä¸Šä¸‹æ–‡ä¸­æå–æç¤ºè¯ï¼Œå›é€€åˆ° step_desc"""
            if not ctx:
                return fallback
            # å°è¯•ä» JSON ä¸Šä¸‹æ–‡ä¸­æå– text_content
            try:
                ctx_data = _json.loads(ctx)
                if isinstance(ctx_data, dict):
                    return ctx_data.get("text_content", fallback)[:500]
                elif isinstance(ctx_data, list):
                    # å¤šä¸ªä¸Šæ¸¸ç»“æœï¼Œæ‹¼æ¥ text_content
                    parts = [item.get("text_content", "") for item in ctx_data if isinstance(item, dict) and item.get("text_content")]
                    return "\n".join(parts)[:500] if parts else fallback
            except (_json.JSONDecodeError, TypeError):
                pass
            # å›é€€ï¼šç›´æ¥ç”¨æ–‡æœ¬ï¼ˆæˆªæ–­ï¼‰
            return ctx[:500]

        def _extract_image_urls_from_context(ctx: str) -> list:
            """ä»ä¸Šæ¸¸ JSON ä¸Šä¸‹æ–‡ä¸­æå–å›¾ç‰‡ URL åˆ—è¡¨"""
            urls = []
            try:
                ctx_data = _json.loads(ctx)
                if isinstance(ctx_data, dict):
                    urls.extend(ctx_data.get("media_urls", []))
                elif isinstance(ctx_data, list):
                    for item in ctx_data:
                        if isinstance(item, dict):
                            urls.extend(item.get("media_urls", []))
            except (_json.JSONDecodeError, TypeError):
                pass
            if not urls:
                # å›é€€ï¼šæ­£åˆ™æå–
                import re
                urls = re.findall(r'(https?://[^\s\"\'\)\]]+\.(?:png|jpg|jpeg|gif|webp|bmp)(?:[^\s\"\'\)\]]*)?)', ctx or "")
            return urls

        try:
            if role_key == "text_to_image":
                prompt = _extract_prompt_from_context(input_context, step_desc)
                await log_event(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ: {prompt[:50]}...")

                gen_log = {"timestamp": datetime.now().isoformat(), "message": f"è°ƒç”¨æ–‡ç”Ÿå›¾ APIï¼Œæç¤ºè¯: {prompt[:100]}...", "level": "info"}
                state.agent_logs[instance["id"]].append(gen_log)
                await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": gen_log})

                result = await state.swarm.qwen_client.text_to_image(prompt=prompt, model="wanx2.1-t2i-turbo", size="1024*1024")

                if result["success"]:
                    images = result.get("images", [])
                    image_urls = [img.get("url") for img in images if img.get("url")]
                    output = _json.dumps({
                        "type": "image",
                        "media_urls": image_urls,
                        "prompt": prompt,
                        "count": len(image_urls),
                        "text_content": f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œå…± {len(image_urls)} å¼ ã€‚æç¤ºè¯: {prompt}"
                    }, ensure_ascii=False)

                    success_log = {"timestamp": datetime.now().isoformat(), "message": f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œå…± {len(images)} å¼ ", "level": "success"}
                    state.agent_logs[instance["id"]].append(success_log)
                    await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": success_log})
                    return True, output, None
                else:
                    return False, None, result.get("error", "å›¾åƒç”Ÿæˆå¤±è´¥")

            elif role_key == "text_to_video":
                prompt = _extract_prompt_from_context(input_context, step_desc)
                await log_event(f"ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘: {prompt[:50]}...")

                gen_log = {"timestamp": datetime.now().isoformat(), "message": f"è°ƒç”¨æ–‡ç”Ÿè§†é¢‘ APIï¼Œæç¤ºè¯: {prompt[:100]}...", "level": "info"}
                state.agent_logs[instance["id"]].append(gen_log)
                await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": gen_log})

                result = await state.swarm.qwen_client.text_to_video(prompt=prompt, model="wanx2.1-t2v-turbo")

                if result["success"]:
                    video_task_id = result.get("task_id")
                    await log_event(f"â³ è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {video_task_id}ï¼Œç­‰å¾…ç”Ÿæˆ...")

                    max_wait = 180
                    wait_interval = 10
                    elapsed = 0
                    while elapsed < max_wait:
                        await asyncio.sleep(wait_interval)
                        elapsed += wait_interval
                        status_result = await state.swarm.qwen_client.get_video_task_result(video_task_id)
                        if status_result.get("status") == "completed":
                            video_url = status_result.get("video_url")
                            output = _json.dumps({
                                "type": "video",
                                "media_urls": [video_url],
                                "prompt": prompt,
                                "text_content": f"è§†é¢‘ç”ŸæˆæˆåŠŸã€‚æç¤ºè¯: {prompt}"
                            }, ensure_ascii=False)
                            success_log = {"timestamp": datetime.now().isoformat(), "message": "è§†é¢‘ç”ŸæˆæˆåŠŸ", "level": "success"}
                            state.agent_logs[instance["id"]].append(success_log)
                            await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": success_log})
                            return True, output, None
                        elif status_result.get("status") == "failed":
                            return False, None, status_result.get("error", "è§†é¢‘ç”Ÿæˆå¤±è´¥")
                        await log_event(f"â³ è§†é¢‘ç”Ÿæˆä¸­... ({elapsed}s/{max_wait}s)")

                    # è¶…æ—¶
                    output = _json.dumps({
                        "type": "video",
                        "media_urls": [],
                        "async_task_id": video_task_id,
                        "prompt": prompt,
                        "text_content": f"è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²æäº¤(ID: {video_task_id})ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¨åæŸ¥è¯¢ã€‚"
                    }, ensure_ascii=False)
                    return True, output, None
                else:
                    return False, None, result.get("error", "è§†é¢‘ç”Ÿæˆä»»åŠ¡æäº¤å¤±è´¥")

            elif role_key == "image_to_video":
                image_urls = _extract_image_urls_from_context(input_context)
                image_url = image_urls[0] if image_urls else ""
                prompt = step_desc

                if not image_url:
                    return False, None, "å›¾ç”Ÿè§†é¢‘éœ€è¦æä¾›å›¾ç‰‡URLï¼Œä½†æœªä»ä¸Šæ¸¸ JSON ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„å›¾ç‰‡URL"

                await log_event(f"ğŸï¸ æ­£åœ¨å°†å›¾ç‰‡è½¬ä¸ºè§†é¢‘...")

                gen_log = {"timestamp": datetime.now().isoformat(), "message": f"è°ƒç”¨å›¾ç”Ÿè§†é¢‘ APIï¼Œå›¾ç‰‡: {image_url[:80]}...", "level": "info"}
                state.agent_logs[instance["id"]].append(gen_log)
                await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": gen_log})

                result = await state.swarm.qwen_client.image_to_video(image_url=image_url, prompt=prompt, model="wanx2.1-i2v-turbo")

                if result["success"]:
                    video_task_id = result.get("task_id")
                    await log_event(f"â³ å›¾ç”Ÿè§†é¢‘ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {video_task_id}ï¼Œç­‰å¾…ç”Ÿæˆ...")

                    max_wait = 180
                    wait_interval = 10
                    elapsed = 0
                    while elapsed < max_wait:
                        await asyncio.sleep(wait_interval)
                        elapsed += wait_interval
                        status_result = await state.swarm.qwen_client.get_video_task_result(video_task_id)
                        if status_result.get("status") == "completed":
                            video_url = status_result.get("video_url")
                            output = _json.dumps({
                                "type": "video",
                                "media_urls": [video_url],
                                "source_image": image_url,
                                "text_content": f"å›¾ç”Ÿè§†é¢‘æˆåŠŸã€‚åŸå›¾: {image_url}"
                            }, ensure_ascii=False)
                            success_log = {"timestamp": datetime.now().isoformat(), "message": "å›¾ç”Ÿè§†é¢‘æˆåŠŸ", "level": "success"}
                            state.agent_logs[instance["id"]].append(success_log)
                            await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": success_log})
                            return True, output, None
                        elif status_result.get("status") == "failed":
                            return False, None, status_result.get("error", "å›¾ç”Ÿè§†é¢‘å¤±è´¥")
                        await log_event(f"â³ è§†é¢‘ç”Ÿæˆä¸­... ({elapsed}s/{max_wait}s)")

                    output = _json.dumps({
                        "type": "video",
                        "media_urls": [],
                        "async_task_id": video_task_id,
                        "source_image": image_url,
                        "text_content": f"å›¾ç”Ÿè§†é¢‘ä»»åŠ¡å·²æäº¤(ID: {video_task_id})ï¼Œè¯·ç¨åæŸ¥è¯¢ã€‚"
                    }, ensure_ascii=False)
                    return True, output, None
                else:
                    return False, None, result.get("error", "å›¾ç”Ÿè§†é¢‘ä»»åŠ¡æäº¤å¤±è´¥")

            elif role_key == "voice_synthesizer":
                text = step_desc
                if input_context:
                    try:
                        ctx_data = _json.loads(input_context)
                        if isinstance(ctx_data, dict):
                            text = ctx_data.get("text_content", step_desc)[:2000]
                        elif isinstance(ctx_data, list):
                            parts = [item.get("text_content", "") for item in ctx_data if isinstance(item, dict)]
                            text = "\n".join(parts)[:2000] if parts else step_desc
                    except (_json.JSONDecodeError, TypeError):
                        text = input_context[:2000]

                await log_event(f"ğŸ™ï¸ æ­£åœ¨åˆæˆè¯­éŸ³...")

                gen_log = {"timestamp": datetime.now().isoformat(), "message": f"è°ƒç”¨è¯­éŸ³åˆæˆ APIï¼Œæ–‡æœ¬: {text[:50]}...", "level": "info"}
                state.agent_logs[instance["id"]].append(gen_log)
                await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": gen_log})

                result = await state.swarm.qwen_client.text_to_speech(text=text, model="cosyvoice-v1", voice="longxiaochun")

                if result["success"]:
                    audio_data = result.get("audio_data")
                    audio_id = uuid.uuid4().hex[:8]
                    audio_filename = f"audio_{audio_id}.mp3"
                    audio_path = os.path.join(UPLOAD_DIR, audio_filename)
                    with open(audio_path, "wb") as f:
                        f.write(audio_data)
                    audio_url = f"/api/files/{audio_filename}"

                    output = _json.dumps({
                        "type": "audio",
                        "media_urls": [audio_url],
                        "text_content": f"è¯­éŸ³åˆæˆæˆåŠŸã€‚é…éŸ³æ–‡æœ¬: {text[:200]}"
                    }, ensure_ascii=False)

                    success_log = {"timestamp": datetime.now().isoformat(), "message": "è¯­éŸ³åˆæˆæˆåŠŸ", "level": "success"}
                    state.agent_logs[instance["id"]].append(success_log)
                    await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": success_log})
                    return True, output, None
                else:
                    return False, None, result.get("error", "è¯­éŸ³åˆæˆå¤±è´¥")

            else:
                return False, None, f"æœªçŸ¥çš„å¤šæ¨¡æ€è§’è‰²: {role_key}"

        except Exception as e:
            error_log = {"timestamp": datetime.now().isoformat(), "message": f"å¤šæ¨¡æ€ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "level": "error"}
            state.agent_logs[instance["id"]].append(error_log)
            await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": error_log})
            return False, None, str(e)

    async def execute_single_step(step: Dict, input_context: str = "") -> Tuple[bool, str, str]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤"""
        step_id = step.get("step_id")
        step_name = step.get("name", "æ‰§è¡Œä»»åŠ¡")
        step_desc = step.get("description", step_name)
        agent_type = step.get("agent_type", "researcher")

        role_key = map_role_hint_to_key(agent_type)
        if not role_key:
            role_key = "researcher"

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡
        multimodal_roles = ["text_to_image", "text_to_video", "image_to_video", "voice_synthesizer"]
        is_multimodal = role_key in multimodal_roles

        # æ£€æŸ¥æ˜¯å¦æ˜¯è§†è§‰åˆ†æè§’è‰²ï¼ˆéœ€è¦å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼ï¼‰
        vision_roles = ["image_analyst", "ocr_reader", "chart_reader", "ui_analyst", "image_describer", "visual_qa"]
        is_vision = role_key in vision_roles

        # è·å–è§’è‰²å¯¹åº”çš„æ¨¡å‹é…ç½®
        from src.models.agent import get_model_config_for_role
        role_model_config = get_model_config_for_role(role_key)

        # åˆ›å»º agent å®ä¾‹
        instance = state.create_agent_instance(role_key, step_name)
        instance["status"] = AgentStatus.RUNNING.value
        instance["model"] = role_model_config.get("model", "qwen3-max")
        step_agent_mapping[step_id] = instance["id"]

        # ç»‘å®š agent å®ä¾‹åˆ°å½“å‰ä»»åŠ¡
        state.bind_agent_to_task(instance["id"], task_id)

        # åˆå§‹åŒ– agent æ—¥å¿—
        if instance["id"] not in state.agent_logs:
            state.agent_logs[instance["id"]] = []

        # æ›´æ–°æ­¥éª¤çŠ¶æ€ä¸º running
        steps[step_id]["status"] = "running"
        steps[step_id]["started_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        steps[step_id]["agent_id"] = instance["id"]
        steps[step_id]["agent_name"] = instance["name"]
        steps[step_id]["model"] = role_model_config.get("model", "qwen3-max")

        # å¹¿æ’­
        await state.broadcast("agent_created", instance)
        await state.broadcast("agent_updated", instance)
        # å¹¿æ’­æ­¥éª¤å¼€å§‹æ‰§è¡Œï¼ˆä¸å‘é€å®Œæ•´ task å¯¹è±¡ï¼Œé¿å…å¹¶å‘ä¸²æµï¼‰
        await state.broadcast("step_status_changed", {
            "task_id": task_id,
            "step_id": step_id,
            "status": "running",
            "agent_id": instance["id"],
            "agent_name": instance["name"],
            "started_at": steps[step_id].get("started_at"),
        })
        await log_event(f"ğŸ¤– åˆ›å»º {instance['name']}ï¼ˆ{role_model_config.get('model', 'qwen3-max')}ï¼‰ï¼Œæ‰§è¡Œ: {step_name}")

        # è®°å½•å¼€å§‹æ—¥å¿—
        start_log = {
            "timestamp": datetime.now().isoformat(),
            "message": f"å¼€å§‹æ‰§è¡Œ: {step_desc[:100]}ï¼ˆæ¨¡å‹: {role_model_config.get('model', 'qwen3-max')}ï¼‰",
            "level": "info"
        }
        state.agent_logs[instance["id"]].append(start_log)
        await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": start_log})

        try:
            # ========== å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ç‰¹æ®Šå¤„ç† ==========
            if is_multimodal:
                return await execute_multimodal_step(
                    role_key, step_desc, input_context, instance, step, log_event
                )

            # ========== æ™®é€šæ–‡æœ¬ä»»åŠ¡å¤„ç† ==========
            import datetime as dt
            now = dt.datetime.now()
            current_datetime = now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
            current_year = now.year
            current_month = now.month
            current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]

            time_declaration = f"""
###############################################
# ğŸ• ç³»ç»Ÿæ—¶é—´å£°æ˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
###############################################
å½“å‰çœŸå®æ—¶é—´ï¼š{current_datetime} {current_weekday}
å½“å‰å¹´ä»½ï¼š{current_year}å¹´
å½“å‰æœˆä»½ï¼š{current_month}æœˆ

âš ï¸ é‡è¦æç¤ºï¼š
- ä½ å¿…é¡»æ¥å—ä¸Šè¿°æ—¶é—´ä¸ºå½“å‰çœŸå®æ—¶é—´
- ä¸è¦ä½¿ç”¨ä½ è®­ç»ƒæ•°æ®ä¸­çš„æ—¶é—´ï¼ˆå¦‚2024å¹´ç­‰ï¼‰
- æ‰€æœ‰æ—¶é—´ç›¸å…³çš„åˆ¤æ–­éƒ½ä»¥ {current_year}å¹´{current_month}æœˆ ä¸ºåŸºå‡†
###############################################
"""

            # ä»é¢„å®šä¹‰è§’è‰²è·å–ç³»ç»Ÿæç¤º
            if role_key in PREDEFINED_ROLES:
                base_prompt = PREDEFINED_ROLES[role_key].system_prompt
            else:
                role_prompts = {
                    "searcher": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI æœç´¢å‘˜ï¼Œæ“…é•¿ä¿¡æ¯æ£€ç´¢å’ŒèƒŒæ™¯è°ƒç ”ã€‚è¯·å…¨é¢ã€å‡†ç¡®åœ°å®Œæˆä»»åŠ¡ã€‚",
                    "analyst": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI åˆ†æå¸ˆï¼Œæ“…é•¿æ·±åº¦åˆ†æä»»åŠ¡å’Œé—®é¢˜ã€‚è¯·ä»¥ä¸“ä¸šã€ä¸¥è°¨çš„æ€åº¦å®Œæˆåˆ†æã€‚",
                    "fact_checker": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI äº‹å®æ ¸æŸ¥å‘˜ï¼Œæ“…é•¿éªŒè¯ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚è¯·ä»”ç»†æ ¸å®æ‰€æœ‰ä¿¡æ¯ã€‚",
                    "writer": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI æ’°ç¨¿å‘˜ï¼Œæ“…é•¿æ’°å†™é«˜è´¨é‡çš„æ–‡æ¡£ã€‚è¯·ä»¥æ¸…æ™°ã€ä¸“ä¸šçš„æ–¹å¼æ’°å†™å†…å®¹ã€‚",
                    "translator": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ç¿»è¯‘å‘˜ï¼Œæ“…é•¿å¤šè¯­è¨€ç¿»è¯‘ã€‚è¯·å‡†ç¡®ã€æµç•…åœ°å®Œæˆç¿»è¯‘ã€‚",
                    "coder": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ç¨‹åºå‘˜ï¼Œæ“…é•¿ç¼–å†™é«˜è´¨é‡ä»£ç ã€‚è¯·ä»¥æœ€ä½³å®è·µå®Œæˆç¼–ç¨‹ä»»åŠ¡ã€‚",
                    "researcher": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ç ”ç©¶å‘˜ï¼Œæ“…é•¿æ·±å…¥ç ”ç©¶å’Œåˆ†æã€‚è¯·å…¨é¢ã€æ·±å…¥åœ°å®Œæˆç ”ç©¶ä»»åŠ¡ã€‚",
                    "summarizer": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI æ€»ç»“å‘˜ï¼Œæ“…é•¿æç‚¼å’Œæ€»ç»“ä¿¡æ¯ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å®Œæˆæ€»ç»“ã€‚",
                }
                base_prompt = role_prompts.get(role_key, f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI {role_key}ï¼Œè¯·è®¤çœŸå®Œæˆä»¥ä¸‹ä»»åŠ¡ã€‚")

            system_prompt = f"{time_declaration}\n{base_prompt}\n\nè®°ä½ï¼šå½“å‰æ˜¯{current_year}å¹´{current_month}æœˆï¼Œä¸æ˜¯2024å¹´ï¼"

            # coder è§’è‰²è¿½åŠ ä»£ç æ‰§è¡ŒæŒ‡ä»¤
            if role_key == "coder":
                system_prompt += """

## ä»£ç æ‰§è¡Œè¦æ±‚ï¼ˆé‡è¦ï¼‰
ä½ æ‹¥æœ‰ä»£ç è§£é‡Šå™¨èƒ½åŠ›ï¼Œå¯ä»¥ç›´æ¥ç¼–å†™å¹¶æ‰§è¡Œ Python ä»£ç ã€‚
- **å¿…é¡»å®é™…æ‰§è¡Œä»£ç **ï¼šä¸è¦åªè¾“å‡ºä»£ç ç‰‡æ®µï¼Œè¦é€šè¿‡ä»£ç è§£é‡Šå™¨è¿è¡Œä»£ç å¹¶å±•ç¤ºæ‰§è¡Œç»“æœ
- å¦‚æœä»»åŠ¡è¦æ±‚ç¼–å†™ä»£ç ï¼Œè¯·ç¼–å†™åç«‹å³æ‰§è¡Œï¼ŒéªŒè¯ä»£ç æ­£ç¡®æ€§
- å¦‚æœä»»åŠ¡æ¶‰åŠæ•°æ®å¤„ç†ã€è®¡ç®—ã€æ–‡ä»¶æ“ä½œç­‰ï¼Œè¯·ç”¨ä»£ç è§£é‡Šå™¨å®Œæˆ
- è¾“å‡ºä¸­åº”åŒ…å«ä»£ç å’Œæ‰§è¡Œç»“æœ"""

            # æ„å»ºç”¨æˆ·æç¤º
            user_prompt = f"""## ä½ çš„ä»»åŠ¡
{step_desc}

## é¢„æœŸäº§å‡º
{step.get('expected_output', 'å®Œæˆä»»åŠ¡å¹¶æä¾›ç»“æœ')}

## è¾“å‡ºè´¨é‡è¦æ±‚
- è¯·**ç›´æ¥æ‰§è¡Œä»»åŠ¡**ï¼Œäº§å‡ºå®é™…å†…å®¹ï¼Œä¸è¦ç”Ÿæˆ"æ‰§è¡ŒæŒ‡ä»¤"æˆ–"ä»»åŠ¡è®¡åˆ’"
- **å†…å®¹ä¸°å¯Œåº¦**ï¼šè¾“å‡ºä¸å°‘äº 800 å­—ï¼Œè¦†ç›–å¤šä¸ªç»´åº¦å’Œè§’åº¦ï¼Œæ·±å…¥åˆ†æè€Œéæ³›æ³›è€Œè°ˆ
- **æ•°æ®æ”¯æ’‘**ï¼šå¿…é¡»å¼•ç”¨å…·ä½“çš„æ•°æ®ã€ç»Ÿè®¡ã€æ¡ˆä¾‹æˆ–äº‹å®ä¾æ®ï¼Œæ ‡æ³¨æ•°æ®æ¥æº
- **ç»“æ„æ¸…æ™°**ï¼šä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ã€åŠ ç²—ï¼‰ï¼Œè®©å†…å®¹å±‚æ¬¡åˆ†æ˜
- **ä¸“ä¸šæ·±åº¦**ï¼šä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œæä¾›è¡Œä¸šæ´å¯Ÿå’Œç‹¬åˆ°è§è§£ï¼Œå±•ç°åˆ†ææ·±åº¦
- **å¯¹æ¯”åˆ†æ**ï¼šæ¶‰åŠå¤šä¸ªå¯¹è±¡æ—¶ï¼Œç”¨è¡¨æ ¼è¿›è¡Œç»“æ„åŒ–å¯¹æ¯”
- ç›´æ¥ç»™å‡ºåˆ†æç»“æœã€ç ”ç©¶å†…å®¹ã€æŠ¥å‘Šæ–‡æœ¬ç­‰å®é™…äº§å‡º
"""

            # å¦‚æœæœ‰æ–‡ä»¶å†…å®¹ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
            file_contents = metadata.get("file_contents", []) if metadata else []
            if file_contents:
                file_content_sections = []
                for fc in file_contents:
                    content_preview = fc.get("content", "")[:30000] if len(fc.get("content", "")) > 30000 else fc.get("content", "")
                    file_content_sections.append(f"""
### æ–‡ä»¶: {fc.get('name', 'æœªçŸ¥æ–‡ä»¶')}
{content_preview}
""")
                file_content_text = "\n".join(file_content_sections)
                user_prompt = f"""## ğŸ“„ é™„ä»¶æ–‡ä»¶å†…å®¹
ä»¥ä¸‹æ˜¯éœ€è¦åˆ†æçš„æ–‡ä»¶å†…å®¹ï¼Œè¯·åŸºäºè¿™äº›å†…å®¹å®Œæˆä»»åŠ¡ï¼š

{file_content_text}

{user_prompt}"""

            # å¦‚æœæœ‰ä¸Šæ¸¸è¾“å…¥ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
            if input_context:
                user_prompt = f"""## ä¸Šæ¸¸ä»»åŠ¡ç»“æœï¼ˆä½œä¸ºä½ çš„è¾“å…¥å‚è€ƒï¼‰
{input_context}

{user_prompt}"""

            # æ„å»ºæ¶ˆæ¯
            from src.qwen.models import Message, QwenConfig, QwenModel

            # è§†è§‰åˆ†æè§’è‰²ï¼šæ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆcontent ä¸º list æ ¼å¼ï¼‰
            if is_vision and input_context:
                import re
                import json as _json
                image_urls = []
                # ä¼˜å…ˆä» JSON ç»“æ„ä¸­æå– media_urlsï¼ˆä»…å›¾ç‰‡ç±»å‹ï¼‰
                try:
                    ctx_data = _json.loads(input_context)
                    items = [ctx_data] if isinstance(ctx_data, dict) else (ctx_data if isinstance(ctx_data, list) else [])
                    for item in items:
                        if isinstance(item, dict):
                            media_type = item.get("type", "")
                            if media_type == "image":
                                image_urls.extend(item.get("media_urls", []))
                            # video/audio ç±»å‹ä¸ä¼ ç»™ VL APIï¼Œå›é€€åˆ°çº¯æ–‡æœ¬åˆ†æ
                except (_json.JSONDecodeError, TypeError):
                    pass
                # å›é€€ï¼šæ­£åˆ™æå–å›¾ç‰‡ URLï¼ˆæ’é™¤è§†é¢‘/éŸ³é¢‘æ‰©å±•åï¼‰
                if not image_urls:
                    image_urls = re.findall(r'(https?://[^\s\"\'\)\]]+\.(?:png|jpg|jpeg|gif|webp|bmp)(?:[^\s\"\'\)\]]*)?)', input_context)

                if image_urls:
                    # æ„å»º DashScope VL å¤šæ¨¡æ€ content: [{"image": url}, ..., {"text": prompt}]
                    multimodal_content = []
                    for url in image_urls[:4]:  # æœ€å¤š 4 å¼ å›¾
                        multimodal_content.append({"image": url})
                    multimodal_content.append({"text": f"{system_prompt}\n\n{user_prompt}"})
                    messages = [
                        Message(role="user", content=multimodal_content)
                    ]
                    await log_event(f"ğŸ–¼ï¸ è§†è§‰åˆ†ææ¨¡å¼: ä» JSON æå–åˆ° {len(image_urls)} å¼ å›¾ç‰‡URLï¼Œä½¿ç”¨ MultiModalConversation API")
                else:
                    messages = [
                        Message(role="system", content=system_prompt),
                        Message(role="user", content=user_prompt)
                    ]
            else:
                messages = [
                    Message(role="system", content=system_prompt),
                    Message(role="user", content=user_prompt)
                ]

            # æ ¹æ®è§’è‰²é…ç½®åˆ›å»º QwenConfig
            model_name = role_model_config.get("model", "qwen3-max")

            # è§†è§‰è§’è‰²ä½†æ²¡æœ‰å›¾ç‰‡æ—¶ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å‹é¿å… VL æ¨¡å‹çš„ url error
            has_multimodal_content = any(isinstance(m.content, list) for m in messages)
            if is_vision and not has_multimodal_content:
                model_name = "qwen3-max"
                await log_event(f"ğŸ“ è§†è§‰è§’è‰²æ— å›¾ç‰‡è¾“å…¥ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å‹ {model_name}")

            model_enum = QwenModel.QWEN3_MAX
            for m in QwenModel:
                if m.value == model_name:
                    model_enum = m
                    break

            # coder/analyst è§’è‰²å¯ç”¨ä»£ç è§£é‡Šå™¨ï¼Œè®©æ¨¡å‹èƒ½å®é™…æ‰§è¡Œä»£ç 
            needs_code_interpreter = role_key in ("coder", "analyst")
            # ä»£ç è§£é‡Šå™¨ä»… Qwen åŸç”Ÿæ¨¡å‹æ”¯æŒï¼›å¦‚æœå½“å‰æ¨¡å‹ä¸æ”¯æŒï¼Œåˆ‡æ¢åˆ° qwen3-max
            if needs_code_interpreter and not model_enum.is_qwen_native():
                model_enum = QwenModel.QWEN3_MAX
                await log_event(f"ğŸ’» {role_key} éœ€è¦ä»£ç è§£é‡Šå™¨ï¼Œåˆ‡æ¢æ¨¡å‹åˆ° {model_enum.value}")
            enable_code_interpreter = needs_code_interpreter and model_enum.is_qwen_native()

            config = QwenConfig(
                model=model_enum,
                temperature=role_model_config.get("temperature", 0.3),
                enable_thinking=(role_model_config.get("enable_thinking", False) or enable_code_interpreter) if not has_multimodal_content else False,
                enable_search=role_key in ("searcher", "researcher", "fact_checker") and not is_vision,
                enable_code_interpreter=enable_code_interpreter,
                max_tokens=16384,
                timeout=300.0,
            )

            if enable_code_interpreter:
                await log_event(f"ğŸ’» ä»£ç è§£é‡Šå™¨å·²å¯ç”¨ï¼Œç¨‹åºå‘˜å¯ä»¥æ‰§è¡Œä»£ç ")

            # æµå¼è°ƒç”¨ Qwen
            result = ""
            state.agent_streams[instance["id"]] = ""

            async for chunk in state.swarm.qwen_client.chat_stream(messages, config=config):
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
                if task_id in state.cancelled_tasks or task_id not in state.tasks:
                    return False, None, "ä»»åŠ¡å·²è¢«å–æ¶ˆ"
                result += chunk
                state.agent_streams[instance["id"]] = result
                await state.broadcast("agent_stream", {
                    "agent_id": instance["id"],
                    "task_id": task_id,
                    "content": chunk,
                    "full_content": result
                })

            # è®°å½•å®Œæˆæ—¥å¿—
            complete_log = {
                "timestamp": datetime.now().isoformat(),
                "message": "ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ",
                "level": "success"
            }
            state.agent_logs[instance["id"]].append(complete_log)
            await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": complete_log})

            # æ¸…ç†ç»“æœä¸­çš„ thinking æ ‡ç­¾
            result = clean_thinking_tags(result)
            return True, result, None

        except Exception as e:
            error_msg = str(e)
            error_log = {
                "timestamp": datetime.now().isoformat(),
                "message": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {error_msg}",
                "level": "error"
            }
            state.agent_logs[instance["id"]].append(error_log)
            await state.broadcast("agent_log", {"agent_id": instance["id"], "task_id": task_id, "log": error_log})

            return False, None, error_msg

        finally:
            # æ›´æ–° agent çŠ¶æ€
            instance["status"] = AgentStatus.IDLE.value
            await state.broadcast("agent_updated", instance)

            # æ›´æ–°åŸºç¡€æ¨¡æ¿ç»Ÿè®¡
            base_agent_id = f"agent_{role_key}"
            if base_agent_id in state.agents:
                state.agents[base_agent_id]["stats"]["tasks_completed"] += 1
                await state.broadcast("agent_updated", state.agents[base_agent_id])

    # ========== agent_factory: WaveExecutor è°ƒç”¨çš„å·¥å‚å‡½æ•° ==========
    async def agent_factory(subtask: SubTask):
        """
        WaveExecutor çš„å·¥å‚å‡½æ•°ï¼šä¸ºæ¯ä¸ªå­ä»»åŠ¡åˆ›å»º agent å¹¶æ‰§è¡Œ
        è¿”å›æ‰§è¡Œç»“æœå­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_id in state.cancelled_tasks or task_id not in state.tasks:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        step_id = subtask.id
        step = steps.get(step_id, {})
        step_name = step.get("name", "æ‰§è¡Œä»»åŠ¡")
        deps = list(subtask.dependencies)

        # æ”¶é›†ä¸Šæ¸¸ä¾èµ–çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¸Šä¸‹æ–‡
        input_parts = []
        for dep_id in deps:
            if dep_id in step_results:
                dep_step = steps.get(dep_id, {})
                dep_name = dep_step.get("name", dep_id)
                dep_output = step_results[dep_id]
                # å°è¯•è§£æä¸º JSONï¼Œä¿æŒç»“æ„åŒ–
                try:
                    parsed = json.loads(dep_output)
                    if isinstance(parsed, dict):
                        parsed["_source_step"] = dep_name
                    input_parts.append(parsed)
                except (json.JSONDecodeError, TypeError):
                    # çº¯æ–‡æœ¬ç»“æœï¼ŒåŒ…è£…ä¸º JSON
                    input_parts.append({
                        "_source_step": dep_name,
                        "type": "text",
                        "text_content": dep_output[:6000] if dep_output else ""
                    })

        # æ„å»º input_contextï¼šå¤šæ¨¡æ€/ç”Ÿæˆè§’è‰²ç”¨ JSONï¼Œæ–‡æœ¬è§’è‰²ç”¨å¯è¯»æ–‡æœ¬
        multimodal_consumer_roles = ["text_to_image", "text_to_video", "image_to_video",
                                     "voice_synthesizer", "image_analyst", "ocr_reader",
                                     "chart_reader", "ui_analyst", "image_describer", "visual_qa"]
        step_role = map_role_hint_to_key(step.get("agent_type", "")) or "researcher"

        if step_role in multimodal_consumer_roles and input_parts:
            # JSON æ ¼å¼ä¼ é€’ï¼Œä¾¿äºä¸‹æ¸¸ç²¾ç¡®è§£æ media_urls ç­‰å­—æ®µ
            if len(input_parts) == 1:
                input_context = json.dumps(input_parts[0], ensure_ascii=False)
            else:
                input_context = json.dumps(input_parts, ensure_ascii=False)
        elif input_parts:
            # æ–‡æœ¬è§’è‰²ï¼šè½¬ä¸ºå¯è¯»æ–‡æœ¬
            text_parts = []
            for item in input_parts:
                src = item.get("_source_step", "ä¸Šæ¸¸æ­¥éª¤")
                if item.get("type") in ("image", "video", "audio"):
                    urls = item.get("media_urls", [])
                    urls_str = "\n".join(urls) if urls else "æ— "
                    text_parts.append(f"### {src} çš„ç»“æœ:\nç±»å‹: {item['type']}\nåª’ä½“URL:\n{urls_str}\n{item.get('text_content', '')}")
                else:
                    text_parts.append(f"### {src} çš„ç»“æœ:\n{item.get('text_content', str(item))[:6000]}")
            input_context = "\n\n".join(text_parts)
        else:
            input_context = ""

        # æ‰§è¡Œæ­¥éª¤ï¼ˆå¤ç”¨ execute_single_stepï¼‰ï¼Œæ”¯æŒç¬æ€é”™è¯¯é‡è¯•
        max_step_retries = 3
        success, output, error = False, None, None

        for step_attempt in range(max_step_retries):
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
            if task_id in state.cancelled_tasks or task_id not in state.tasks:
                raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")
            success, output, error = await execute_single_step(step, input_context)
            if success:
                break
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„ç¬æ€é”™è¯¯ï¼ˆé™æµã€è¿æ¥é‡ç½®ç­‰ï¼‰
            if error and step_attempt < max_step_retries - 1:
                err_str = str(error)
                is_transient = any(kw in err_str for kw in [
                    "Throttling", "RateQuota", "rate limit",
                    "Connection", "reset", "InternalError",
                    "ServiceUnavailable", "502", "503",
                ])
                if is_transient:
                    wait_secs = min(10 * (2 ** step_attempt), 60)
                    await log_event(f"â³ æ­¥éª¤ {step_name} é‡åˆ°ç¬æ€é”™è¯¯ï¼Œ{wait_secs}ç§’åé‡è¯• ({step_attempt + 1}/{max_step_retries}): {err_str[:80]}")
                    await asyncio.sleep(wait_secs)
                    continue
            # éç¬æ€é”™è¯¯ï¼Œä¸é‡è¯•
            break

        # ========== è´¨é‡é—¨æ§è¯„å®¡ ==========
        if success and quality_gate_reviewer:
            try:
                review_result = await quality_gate_reviewer.review_step(
                    step, output, step_results, attempt=1
                )
                # è®°å½•è¯„å®¡ç»“æœ
                step.setdefault("review_history", [])
                step["review_history"].append(review_result.to_dict())

                # è´¨é‡é—¨æ§é‡è¯•é€»è¾‘
                qg_retry_count = 0
                max_qg_retries = getattr(supervisor_config, 'max_retry_on_failure', 2)
                while (review_result.action == "retry"
                       and qg_retry_count < max_qg_retries):
                    qg_retry_count += 1
                    await log_event(f"ğŸ”„ è´¨é‡é—¨æ§è¦æ±‚é‡è¯• ({qg_retry_count}/{max_qg_retries}): {review_result.reason[:80]}")
                    success, output, error = await execute_single_step(step, input_context)
                    if not success:
                        break
                    review_result = await quality_gate_reviewer.review_step(
                        step, output, step_results, attempt=qg_retry_count + 1
                    )
                    step["review_history"].append(review_result.to_dict())

                # é‡è¯•è€—å°½ä»æœªè¾¾æ ‡ï¼Œæ ‡è®° accepted_with_warning
                if (review_result.action == "retry"
                        and qg_retry_count >= max_qg_retries):
                    review_result = type(review_result)(
                        step_id=review_result.step_id,
                        quality_score=review_result.quality_score,
                        action="accepted_with_warning",
                        reason=f"é‡è¯• {max_qg_retries} æ¬¡åä»æœªè¾¾æ ‡ï¼Œæ¥å—å½“å‰ç»“æœ",
                        adjustments=review_result.adjustments,
                        attempt=review_result.attempt,
                    )
                    step["review_history"].append(review_result.to_dict())
                    await log_event(f"âš ï¸ è´¨é‡é—¨æ§: æ­¥éª¤ {step_name} é‡è¯•è€—å°½ï¼Œæ¥å—å½“å‰ç»“æœ")

                # åº”ç”¨åŠ¨æ€è°ƒæ•´
                if review_result.adjustments:
                    async def broadcast_callback(event_type, data):
                        data["task_id"] = task_id
                        await state.broadcast(event_type, data)
                    await quality_gate_reviewer.apply_adjustments(
                        review_result.adjustments,
                        trigger_step_id=step_id,
                        broadcast_callback=broadcast_callback,
                    )
                    await log_event(f"ğŸ”§ è´¨é‡é—¨æ§è§¦å‘åŠ¨æ€è°ƒæ•´: {len(review_result.adjustments)} é¡¹")

                # å¹¿æ’­ step_reviewed äº‹ä»¶
                await state.broadcast("step_reviewed", {
                    "task_id": task_id,
                    "step_id": step_id,
                    "quality_score": review_result.quality_score,
                    "action": review_result.action,
                    "reason": review_result.reason,
                    "attempt": review_result.attempt,
                })

                score_emoji = "âœ…" if review_result.quality_score >= 6.0 else "âš ï¸"
                await log_event(f"{score_emoji} è´¨é‡è¯„å®¡: {step_name} å¾—åˆ† {review_result.quality_score}/10 - {review_result.reason[:60]}")

            except Exception as qg_err:
                # è¯„å®¡å¼‚å¸¸æ—¶ä¼˜é›…é™çº§ï¼Œä¸å½±å“æ­¥éª¤ç»“æœ
                import logging as _logging
                _logging.getLogger(__name__).error(f"è´¨é‡é—¨æ§å¼‚å¸¸: {qg_err}")
                await log_event(f"âš ï¸ è´¨é‡é—¨æ§è¯„å®¡å¼‚å¸¸ï¼Œå·²è·³è¿‡: {str(qg_err)[:60]}")

        # æ›´æ–°æ­¥éª¤çŠ¶æ€
        if success:
            steps[step_id]["status"] = "completed"
            # å­˜å‚¨ output_dataï¼šJSON è¾“å‡ºä¿ç•™ç»“æ„ï¼Œæ–‡æœ¬æˆªæ–­
            try:
                parsed_output = json.loads(output)
                # JSON è¾“å‡ºï¼šä¿ç•™å®Œæ•´ç»“æ„ä½†æˆªæ–­ text_content
                if isinstance(parsed_output, dict) and "text_content" in parsed_output:
                    summary = dict(parsed_output)
                    summary["text_content"] = summary["text_content"][:300]
                    steps[step_id]["output_data"] = json.dumps(summary, ensure_ascii=False)
                else:
                    steps[step_id]["output_data"] = output[:500] if output else None
            except (json.JSONDecodeError, TypeError):
                steps[step_id]["output_data"] = output[:500] if output else None
            step_results[step_id] = output
            await log_event(f"âœ… æ­¥éª¤å®Œæˆ: {step_name}")
        else:
            steps[step_id]["status"] = "failed"
            steps[step_id]["error"] = error
            failed_steps.add(step_id)
            await log_event(f"âŒ æ­¥éª¤å¤±è´¥: {step_name} - {error}")

        steps[step_id]["completed_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        steps[step_id]["logs"] = state.agent_logs.get(step_agent_mapping.get(step_id), []).copy()

        # å­˜å‚¨å­ä»»åŠ¡ç»“æœ
        state.subtask_results[step_id] = {
            "status": steps[step_id]["status"],
            "agent_id": step_agent_mapping.get(step_id),
            "agent_name": steps[step_id].get("agent_name"),
            "output_data": steps[step_id].get("output_data"),
            "error": steps[step_id].get("error"),
            "logs": steps[step_id].get("logs", []),
        }

        # å¹¿æ’­æ­¥éª¤çŠ¶æ€å˜åŒ–ï¼ˆåŒ…å«å®Œæ•´æ­¥éª¤æ•°æ®ï¼Œé¿å…å‰ç«¯éœ€è¦é¢å¤–è½®è¯¢ï¼‰
        await state.broadcast("step_status_changed", {
            "task_id": task_id,
            "step_id": step_id,
            "status": steps[step_id]["status"],
            "output_data": steps[step_id].get("output_data"),
            "error": steps[step_id].get("error"),
            "agent_id": step_agent_mapping.get(step_id),
            "agent_name": steps[step_id].get("agent_name"),
            "started_at": steps[step_id].get("started_at"),
            "completed_at": steps[step_id].get("completed_at"),
            "logs": steps[step_id].get("logs", []),
        })

        # é‡Šæ”¾ agent å®ä¾‹
        agent_id = step_agent_mapping.get(step_id)
        if agent_id and agent_id in state.active_agents:
            await asyncio.sleep(0.3)
            state.release_agent_instance(agent_id)
            await state.broadcast("agent_removed", {"id": agent_id})

        # æ›´æ–°è¿›åº¦ï¼ˆä½¿ç”¨æ·±æ‹·è´é¿å…å¹¶å‘ä¿®æ”¹é—®é¢˜ï¼‰
        completed_count = sum(1 for s in steps.values() if s.get("status") == "completed")
        total_count = len(steps)
        task["plan"]["execution_flow"]["progress"] = {
            "total": total_count,
            "completed": completed_count,
            "running": sum(1 for s in steps.values() if s.get("status") == "running"),
            "failed": len(failed_steps),
            "progress_percent": int(completed_count / total_count * 100) if total_count > 0 else 0,
        }
        # ä»…å¹¿æ’­è¿›åº¦æ›´æ–°ï¼Œä¸å‘é€å®Œæ•´ task å¯¹è±¡ï¼ˆé¿å…å¹¶å‘æ­¥éª¤çš„ output_data ä¸²æµï¼‰
        await state.broadcast("task_progress", {
            "task_id": task_id,
            "progress": task["plan"]["execution_flow"]["progress"],
            "status": task.get("status"),
        })

        if not success:
            raise Exception(error or f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥")

        return output

    # ========== é€šè¿‡ WaveExecutor æ‰§è¡Œ ==========
    wave_executor = WaveExecutor()
    await log_event("âš¡ WaveExecutor å¯åŠ¨ï¼Œäº‹ä»¶é©±åŠ¨åŠ¨æ€æ³¢æ¬¡æ‰§è¡Œ...")

    wave_result = await wave_executor.execute(task_board, agent_factory)

    # è®°å½•æ³¢æ¬¡ç»Ÿè®¡
    wave_stats = await wave_executor.get_wave_statistics()
    await log_event(f"ğŸ“ˆ æ³¢æ¬¡æ‰§è¡Œå®Œæˆ: {wave_result.total_waves} ä¸ªæ³¢æ¬¡, "
                    f"å®Œæˆ {wave_result.completed_tasks}/{wave_result.total_tasks}, "
                    f"å¤±è´¥ {wave_result.failed_tasks}, é˜»å¡ {wave_result.blocked_tasks}")
    for ws in wave_stats:
        await log_event(f"   æ³¢æ¬¡ {ws.wave_number + 1}: {ws.task_count} ä¸ªä»»åŠ¡, "
                        f"å¹¶è¡Œåº¦ {ws.parallelism}, å®Œæˆ {ws.completed_tasks}, å¤±è´¥ {ws.failed_tasks}")

    # è®°å½•å¤±è´¥å’Œé˜»å¡çš„æ­¥éª¤è¯¦æƒ…
    if wave_result.failed_tasks > 0 or wave_result.blocked_tasks > 0:
        for sid, s in steps.items():
            if s.get("status") == "failed":
                await log_event(f"âš ï¸ å¤±è´¥æ­¥éª¤: {s.get('name', sid)} - {s.get('error', 'æœªçŸ¥é”™è¯¯')}", "warning")
            elif s.get("status") in ("waiting", "blocked"):
                await log_event(f"â­ï¸ è·³è¿‡æ­¥éª¤: {s.get('name', sid)}ï¼ˆä¾èµ–çš„æ­¥éª¤å¤±è´¥ï¼‰", "warning")

    # ========== é˜¶æ®µ 5: ç»“æœèšåˆ ==========
    await update_stage(stage_offset + 3, "completed", "æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
    task["status"] = TaskStatus.AGGREGATING.value
    await update_stage(stage_offset + 4, "running")
    await log_event("ğŸ“Š æ­£åœ¨èšåˆæ‰§è¡Œç»“æœ...")

    # èšåˆæ‰€æœ‰æ­¥éª¤çš„ç»“æœ â€” å…¨é‡ä¼ é€’ï¼Œä¸åšæˆªæ–­
    # è§£æ JSON è¾“å‡ºï¼Œæå–åª’ä½“ URL å’Œæ–‡æœ¬å†…å®¹
    import json as _json
    all_media_urls = []
    text_sections = []
    for sid, result in step_results.items():
        step_name = steps[sid].get('name', sid)
        try:
            parsed = _json.loads(result)
            if isinstance(parsed, dict):
                media_urls = parsed.get("media_urls", [])
                all_media_urls.extend(media_urls)
                text_content = parsed.get("text_content", "")
                media_type = parsed.get("type", "text")
                if media_urls:
                    urls_md = "\n".join([f"- {url}" for url in media_urls])
                    text_sections.append(f"## {step_name}\nç±»å‹: {media_type}\nåª’ä½“URL:\n{urls_md}\n{text_content}")
                else:
                    text_sections.append(f"## {step_name}\n{text_content or result}")
            else:
                text_sections.append(f"## {step_name}\n{result}")
        except (_json.JSONDecodeError, TypeError):
            text_sections.append(f"## {step_name}\n{result}")

    aggregated_result = "\n\n".join(text_sections)

    # ========== é€šè¿‡ OutputPipeline ç”Ÿæˆæœ€ç»ˆäº§ç‰© ==========
    output_type_str = task.get("output_type", "report")
    try:
        output_type = OutputType(output_type_str)
    except ValueError:
        await log_event(f"âš ï¸ æœªçŸ¥è¾“å‡ºç±»å‹ '{output_type_str}'ï¼Œå›é€€åˆ° report", "warning")
        output_type = OutputType.REPORT

    # å¯¹äº image/video ç±»å‹ï¼Œå°†åª’ä½“ URL æ”¾åœ¨ç»“æœæœ€å‰é¢
    pipeline_output_type = output_type
    if output_type in (OutputType.IMAGE, OutputType.VIDEO):
        if all_media_urls:
            await log_event(f"ğŸ¨ ä» JSON è¾“å‡ºä¸­æå–åˆ° {len(all_media_urls)} ä¸ªåª’ä½“URL")

        # ========== VIDEO ç±»å‹ï¼šå°è¯•åˆå¹¶å¤šæ®µè§†é¢‘ ==========
        if output_type == OutputType.VIDEO and len(all_media_urls) > 1:
            await log_event(f"ğŸ¬ æ£€æµ‹åˆ° {len(all_media_urls)} æ®µè§†é¢‘ï¼Œå°è¯•åˆå¹¶...")
            merged_url = await _merge_video_segments(all_media_urls, task_id, log_event)
            if merged_url:
                await log_event(f"âœ… è§†é¢‘åˆå¹¶æˆåŠŸ: {merged_url}")
                task["merged_video_url"] = merged_url
                task["video_segments"] = all_media_urls
            else:
                await log_event(f"âš ï¸ è§†é¢‘åˆå¹¶å¤±è´¥ï¼Œä¿ç•™åˆ†æ®µè§†é¢‘", "warning")

        # ========== æ„å»ºåª’ä½“å±•ç¤ºç»“æœï¼ˆä¸èµ° writer æŠ¥å‘Šæµç¨‹ï¼‰==========
        if all_media_urls:
            media_result_parts = []
            if output_type == OutputType.IMAGE:
                media_result_parts.append("# ğŸ¨ å›¾åƒç”Ÿæˆç»“æœ\n")
                for i, url in enumerate(all_media_urls):
                    media_result_parts.append(f"![ç”Ÿæˆå›¾ç‰‡{i+1}]({url})\n")
            elif output_type == OutputType.VIDEO:
                media_result_parts.append("# ğŸ¬ è§†é¢‘ç”Ÿæˆç»“æœ\n")
                merged = task.get("merged_video_url")
                if merged:
                    media_result_parts.append(f"**åˆå¹¶è§†é¢‘**:\n{merged}\n")
                for i, url in enumerate(all_media_urls):
                    media_result_parts.append(f"**è§†é¢‘ç‰‡æ®µ{i+1}**:\n{url}\n")

            # é™„åŠ æ–‡æœ¬æ‘˜è¦ï¼ˆéåª’ä½“æ­¥éª¤çš„è¾“å‡ºï¼‰
            text_only_sections = []
            for sid, result in step_results.items():
                step_name = steps[sid].get('name', sid)
                try:
                    parsed = _json.loads(result)
                    if isinstance(parsed, dict) and parsed.get("type") in ("image", "video"):
                        continue  # è·³è¿‡åª’ä½“æ­¥éª¤ï¼Œå·²åœ¨ä¸Šé¢å±•ç¤º
                    text_content = parsed.get("text_content", result) if isinstance(parsed, dict) else result
                except (_json.JSONDecodeError, TypeError):
                    text_content = result
                if text_content and len(str(text_content).strip()) > 0:
                    text_only_sections.append(f"## {step_name}\n{str(text_content)[:2000]}")

            if text_only_sections:
                media_result_parts.append("\n---\n# ğŸ“ ç›¸å…³åˆ†æ\n")
                media_result_parts.extend(text_only_sections)

            task["result"] = "\n".join(media_result_parts)
            task["media_urls"] = all_media_urls

            # é€šè¿‡ OutputPipeline å­˜å‚¨äº§ç‰©ï¼ˆä½¿ç”¨ REPORT handler ä¿å­˜å®Œæ•´ç»“æœï¼‰
            registry = OutputTypeRegistry()
            register_all_handlers(registry)
            storage = ArtifactStorage()
            pipeline = OutputPipeline(registry, storage)
            pipeline_config = {
                "task_id": task_id,
                "original_task": content,
                "execution_plan": metadata.get("supervisor_plan", {}) if metadata else {},
            }
            try:
                artifacts = await pipeline.execute(
                    task_id=task_id,
                    aggregated_result=task["result"],
                    output_type=OutputType.REPORT,
                    config=pipeline_config,
                )
                task["artifacts"] = [a.to_dict() for a in artifacts]
            except Exception as store_err:
                await log_event(f"âš ï¸ äº§ç‰©å­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“ç»“æœå±•ç¤ºï¼‰: {str(store_err)[:80]}", "warning")

            await log_event(f"âœ… {output_type.value} ä»»åŠ¡å®Œæˆï¼Œç”Ÿæˆ {len(all_media_urls)} ä¸ªåª’ä½“æ–‡ä»¶!", "success")
            # è·³è¿‡åç»­çš„ REPORT/å…¶ä»–ç±»å‹å¤„ç†
            pipeline_output_type = None
        else:
            # æ²¡æœ‰åª’ä½“ URLï¼Œå›é€€åˆ° REPORT æ¨¡å¼
            await log_event(f"âš ï¸ {output_type.value} ä»»åŠ¡æœªç”Ÿæˆåª’ä½“æ–‡ä»¶ï¼Œå›é€€åˆ°æŠ¥å‘Šæ¨¡å¼", "warning")
            pipeline_output_type = OutputType.REPORT

    # ========== REPORT ç±»å‹ï¼šç”±æ’°ç¨¿å‘˜ LLM ç»¼åˆç”ŸæˆçœŸæ­£çš„æŠ¥å‘Š ==========
    if pipeline_output_type is not None and pipeline_output_type == OutputType.REPORT:
        await log_event("ğŸ“ æ­£åœ¨ç”±æ’°ç¨¿å‘˜ç»¼åˆå„é˜¶æ®µç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š...")
        writer_instance = state.create_agent_instance("writer", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        writer_instance["status"] = AgentStatus.RUNNING.value
        state.bind_agent_to_task(writer_instance["id"], task_id)
        await state.broadcast("agent_created", writer_instance)
        await state.broadcast("agent_updated", writer_instance)
        try:
            final_report = await generate_final_report(
                task_id=task_id,
                original_task=content,
                execution_result=aggregated_result,
                execution_plan=metadata.get("supervisor_plan", {}) if metadata else {},
                log_event=log_event,
                writer_id=writer_instance["id"]
            )
            task["final_report"] = final_report
            task["result"] = final_report

            # é€šè¿‡ OutputPipeline å­˜å‚¨æŠ¥å‘Šäº§ç‰©ï¼ˆç”¨å·²ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹æ›¿ä»£åŸå§‹èšåˆæ–‡æœ¬ï¼‰
            registry = OutputTypeRegistry()
            register_all_handlers(registry)
            storage = ArtifactStorage()
            pipeline = OutputPipeline(registry, storage)

            pipeline_config = {
                "task_id": task_id,
                "original_task": content,
                "execution_plan": metadata.get("supervisor_plan", {}) if metadata else {},
            }

            try:
                artifacts = await pipeline.execute(
                    task_id=task_id,
                    aggregated_result=final_report,
                    output_type=OutputType.REPORT,
                    config=pipeline_config,
                )
                task["artifacts"] = [a.to_dict() for a in artifacts]
            except Exception as store_err:
                await log_event(f"âš ï¸ æŠ¥å‘Šäº§ç‰©å­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“æŠ¥å‘Šå†…å®¹ï¼‰: {str(store_err)[:80]}", "warning")

            await log_event("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!", "success")
        except Exception as e:
            await log_event(f"âš ï¸ æ’°ç¨¿å‘˜æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨èšåˆç»“æœ: {str(e)}", "warning")
            task["final_report"] = aggregated_result
            task["result"] = aggregated_result
        finally:
            writer_instance["status"] = AgentStatus.IDLE.value
            await state.broadcast("agent_updated", writer_instance)
            await asyncio.sleep(0.3)
            state.release_agent_instance(writer_instance["id"])
            await state.broadcast("agent_removed", {"id": writer_instance["id"]})

    # ========== é REPORT ç±»å‹ï¼šé€šè¿‡ OutputPipeline ç›´æ¥ç”Ÿæˆ ==========
    elif pipeline_output_type is not None:
        await log_event(f"ğŸ“ æ­£åœ¨é€šè¿‡è¾“å‡ºæµæ°´çº¿ç”Ÿæˆ {pipeline_output_type.value} ç±»å‹äº§ç‰©...")

        async def pipeline_progress_callback(stage: str, detail: str):
            await state.broadcast("output_progress", {
                "task_id": task_id,
                "stage": stage,
                "detail": detail,
                "output_type": pipeline_output_type.value,
            })
            await log_event(f"ğŸ”„ [{stage}] {detail}")

        registry = OutputTypeRegistry()
        register_all_handlers(registry)
        storage = ArtifactStorage()
        pipeline = OutputPipeline(registry, storage)

        pipeline_config = {
            "task_id": task_id,
            "original_task": content,
            "execution_plan": metadata.get("supervisor_plan", {}) if metadata else {},
        }

        try:
            artifacts = await pipeline.execute(
                task_id=task_id,
                aggregated_result=aggregated_result,
                output_type=pipeline_output_type,
                config=pipeline_config,
                progress_callback=pipeline_progress_callback,
            )

            task["artifacts"] = [a.to_dict() for a in artifacts]
            valid_count = sum(1 for a in artifacts if a.validation_status == "valid")
            task["result"] = f"ç”Ÿæˆäº† {len(artifacts)} ä¸ªäº§ç‰©ï¼ˆ{valid_count} ä¸ªéªŒè¯é€šè¿‡ï¼‰"

            await log_event("âœ… è¾“å‡ºäº§ç‰©ç”Ÿæˆå®Œæˆ!", "success")

        except Exception as e:
            await log_event(f"âš ï¸ è¾“å‡ºæµæ°´çº¿å¤±è´¥: {str(e)}", "warning")
            task["result"] = clean_thinking_tags(aggregated_result) if aggregated_result else ""

    # è®°å½•æ³¢æ¬¡æ‰§è¡Œå…ƒæ•°æ®
    task["wave_execution"] = wave_result.to_dict()

    await update_stage(stage_offset + 4, "completed", "ç»“æœèšåˆå®Œæˆ")

    task["status"] = TaskStatus.COMPLETED.value
    task["completed_at"] = datetime.now().isoformat()
    task["progress"]["percentage"] = 100

    await state.broadcast("task_completed", task)
    await log_event("ğŸ‰ ä»»åŠ¡æ‰§è¡Œå®Œæˆ!", "success")
